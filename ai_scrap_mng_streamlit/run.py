import streamlit as st
import pandas as pd
from streamlit_option_menu import option_menu
from streamlit_timeline import timeline

from core.utils import get_scrap_managers, get_scraper_logs, convert_to_timelinejs_format_with_colors, convert_to_timelinejs_format_with_alerts, convert_error_logs_to_timelinejs_format, create_color_map, get_data_from_api
from core.functions import create_form, update_form, delete_form, check_html, scrap_test_beautiful_soup


st.set_page_config(
   page_title="Scrap Manager",
   page_icon="ğŸ§Š",
   layout="wide",
   initial_sidebar_state="expanded",
)


# option = st.sidebar.selectbox(
#     'Menu',
#     ('Scrap Manager', 'Scrap Test', 'Scraper Logs')
# )

with st.sidebar:
    option = option_menu(
        "Menu", ['Scrap Manager', 'Scrap Test', 'Scraper Logs', 'ESG Finance Hub Scraper'],
        icons=['house', 'kanban', 'bi bi-robot', 'bi bi-robot'],
        menu_icon="app-indicator",
        default_index=0,
        styles={
            "container": {"padding": "4!important", "background-color": "#fafafa"},
            "icon": {"color": "black", "font-size": "25px"},
            "nav-link": {"font-size": "16px", "text-align": "left", "margin":"0px", "--hover-color": "#fafafa"},
            "nav-link-selected": {"background-color": "#08c7b4"},
            }
        )


# ìŠ¤í¬ë© ë§¤ë‹ˆì €
if option == 'Scrap Manager':


    st.title("Scrap Manager")

    # ë°ì´í„° ì¡°íšŒ
    st.header("Search Scrap Manager")
    portal = st.selectbox('Portal', (
        'Select',
        'daum',
        'naver',
        'naver_sports',
        'zdnet',
        'venturesquare',
        'the bell',
        'platum',
        'startupn',
        'startuptoday',
        'esg_economy',
        'greenpost_korea'
        ))
    if st.button("Search"):
        scrap_managers = get_scrap_managers(portal)
        df = pd.DataFrame(scrap_managers)

        # 'parsing_rule' ì—´ì´ ì´ë¯¸ dict íƒ€ì…ì´ë¯€ë¡œ ë°”ë¡œ json_normalizeë¥¼ ì‚¬ìš©
        parsing_rules_df = pd.json_normalize(df['parsing_rule'])

        # ì›ë³¸ ë°ì´í„°í”„ë ˆì„ì— ìƒˆ ì—´ ì¶”ê°€
        df = df.join(parsing_rules_df)

        # ì›ë˜ 'parsing_rule' ì—´ ì œê±°
        df.drop(columns=['parsing_rule'], inplace=True)

        # NULL ê°’ì„ Pythonì˜ Noneìœ¼ë¡œ ë³€í™˜
        df.fillna(value=pd.NA, inplace=True)

        # id ì¹¼ëŸ¼ì„ ì¸ë±ìŠ¤ë¡œ ì‚¬ìš©
        df.set_index('id', inplace=True)

        # ìŠ¤íƒ€ì¼ ì ìš©
        styled_df = df.style.set_properties(**{'text-align': 'left'})

        # Streamlitì„ ì‚¬ìš©í•˜ì—¬ ì›¹ ì•±ì— DataFrame í‘œì‹œ
        st.dataframe(styled_df)


    # ë°ì´í„° ì‚½ì…
    st.header("Create New Scrap Manager")
    create_form()


    # ë°ì´í„° ì—…ë°ì´íŠ¸
    st.header("Update Scrap Manager")
    update_form()


    # ë°ì´í„° ì‚­ì œ
    st.header("Delete Scrap Manager")
    delete_form()

# ìŠ¤í¬ë© í…ŒìŠ¤íŠ¸
elif option == 'Scrap Test':
    st.title("Scrap Test")

    # ë°ì´í„° ì¡°íšŒ
    st.header("Scrap Test")
    test_url = st.text_input("Enter test url to scrape:")

    # url ì…ë ¥ í›„ html ê°€ì ¸ì˜¤ê¸° ë° í˜ì´ì§€ ë Œë”ë§
    check_html(test_url)
    
    # ìŠ¤í¬ë˜í•‘ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„ íƒ
    scrap_library = st.selectbox(
        'Scrap Library',
        ('Select', 'BeautifulSoup', 'Selenium(Not Supported Yet)')
    )

    if scrap_library == 'BeautifulSoup':
        scrap_test_beautiful_soup(st.session_state.html)

# ìŠ¤í¬ë˜í¼ ë¡œê·¸
elif option == 'Scraper Logs':
    st.title("Scraper Logs")

    # ë°ì´í„° ì¡°íšŒ
    scraper_logs = get_scraper_logs()
    st.session_state.scrap_session_logs = scraper_logs['scrap_session_logs']
    st.session_state.scrap_error_logs = scraper_logs['scrap_error_logs']

    # pandas ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
    scrap_session_logs_df = pd.DataFrame(st.session_state.scrap_session_logs)
    scrap_error_logs_df = pd.DataFrame(st.session_state.scrap_error_logs)

    st.header("Scrap Session Logs")
    session_logs_filter = st.selectbox(
        'Filter',
        ('Timeline By Portal', 'Timeline By Status', 'Table'),
        index=1
    )
    if session_logs_filter == 'Timeline By Status':
        timelinejs_json = convert_to_timelinejs_format_with_alerts(scrap_session_logs_df)
    elif session_logs_filter == 'Timeline By Portal':
        timelinejs_json = convert_to_timelinejs_format_with_colors(scrap_session_logs_df)
    elif session_logs_filter == 'Table':
        timelinejs_json = None
        st.dataframe(scrap_session_logs_df)

    if timelinejs_json:
        timeline(timelinejs_json, height=500)

    st.header("Scrap Error Logs")
    error_logs_filter = st.selectbox(
        'Filter',
        ('Timeline', 'Table'),
        index=0
    )

    color_map = create_color_map(scrap_error_logs_df)
    default_color = '#808080'

    if error_logs_filter == 'Timeline':
        timelinejs_json = convert_error_logs_to_timelinejs_format(scrap_error_logs_df, color_map, default_color)
        timeline(timelinejs_json, height=500)
    elif error_logs_filter == 'Table':
        st.dataframe(scrap_error_logs_df)

# ESG Finance Hub Scraper
elif option == 'ESG Finance Hub Scraper':
    st.title("ESG Finance Hub Scraper")

    # ë°ì´í„° ì¡°íšŒ
    st.session_state.portal_for_data = st.selectbox('Portal', (
        'Select',
        'esg_finance_hub'
        ), index=1)

    st.session_state.data_page_no = st.number_input(
        'Page No',
        min_value=1,
        max_value=100000,
        value=1
        )

    search_button = st.button("Search", key='search_button_for_data_esg_finance_hub')
    if search_button:
        if st.session_state.portal_for_data == 'esg_finance_hub':
            st.session_state.esg_finance_hub_data_df = get_data_from_api(st.session_state.portal_for_data)
            st.session_state.esg_finance_hub_data_df.columns = ['page', 'news_url']
            st.session_state.esg_finance_hub_data_df['domain'] = st.session_state.esg_finance_hub_data_df['news_url'].apply(lambda x: pd.Series(x.split('/')[2]))
            unique_domains = st.session_state.esg_finance_hub_data_df['domain'].unique()

            st.session_state.data_df = st.session_state.esg_finance_hub_data_df[st.session_state.esg_finance_hub_data_df['page'] == st.session_state.data_page_no]

        st.dataframe(st.session_state.data_df)
        st.write(f"Total: {len(st.session_state.esg_finance_hub_data_df)}")

        # ë„ë©”ì¸ë³„ë¡œ ë‰´ìŠ¤ ê°œìˆ˜ë¥¼ ì„¸ì–´ë´…ë‹ˆë‹¤.
        st.write("Number of news by domain:")
        st.bar_chart(st.session_state.esg_finance_hub_data_df['domain'].value_counts())
        st.write("Total domains:", len(unique_domains))
